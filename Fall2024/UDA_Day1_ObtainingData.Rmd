---
html_document:
  toc: yes
author: "Unstructured Data Analytics, Fall 2024"
output:
  html_document:
    df_print: paged
always_allow_html: yes
title: 'Obtaining Data'
toc_float: yes
theme: spacelab
highlight: tango
df_print: paged
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Loading local files

### Single file, details known

```{r echo=TRUE, message=FALSE, warning=FALSE}

my_file <- ??

```

  
### Exploring using R
See what exists if you were to click to open a folder
```{r}
list.files("C:/Users/alanski/Documents")
```
  
See what exists in here with the full path
```{r}
list.files("C:/Users/alanski/Documents", full.names = TRUE)
```
  
  
See folder names only; can also look inside for subfolders using **recursive**
```{r}
list.dirs("C:/Users/alanski/Documents", recursive = TRUE)
```
### Limit with pattern arguments
```{r}
list.files("C:/Users/alanski/Downloads/", recursive = TRUE, pattern = "Completed.Rmd", full.names = TRUE)
```


### Read in and combine into one dataframe
```{r}
# set up list of file paths
deg_files <- list.files("C:/Users/alanski/Downloads/", pattern = "Biz.csv", full.names = TRUE)


# use purrr to read and combine all at once (it's a loop)
deg_data <- purrr::map(deg_files, read.csv) %>% 
  purrr::map(data.frame)

#take a look at the results
head(deg_data)

```

## Loading files from google drive

### Setup
First, load the packages
```{r}
#(install as needed)
library(googledrive)
library(googlesheets4)
```

### Authenticate
It's nice to use the same authentication for googledrive and googlesheets4

```{r}
#authenticate with google drive: follow the interactive prompts to grant access
drive_auth()
```
```{r}
# then share that authentication with google sheets
gs4_auth(token = drive_token())
```

You're now connected.


### Explore & Interact

Find files anywhere using keywords
```{r}
drive_find("Collaboration",type = "spreadsheet", n_max = 10)
```

Select and load a file into R
```{r}
# file metadata
ss <- drive_get("UDA 2024 Collaboration")

#as a tibble
read_sheet(ss)
```
Make changes, like adding a new row of data
```{r}
sheet_append(ss, data = data.frame(Section = "??", 
                                   Name = "???",
                                   MakesSense = "???"))

read_sheet(ss)
```

Additional functions exist to create entirely new spreadsheets or add sheets within an existing spreadsheet. 
## Simple APIs

### Let's collect quotable insults

Free, open, no-authentication API.   
Documentation here: https://evilinsult.com/api/
  
*Warning: You may experience LANGUAGE*  
  
Load packages
```{r}
library(httr2)
library(jsonlite)
```


Set up and make the request: explore what you get at each stage
```{r}
req <- request("https://evilinsult.com/generate_insult.php?lang=en&type=json")
#check the setup
print(req)

#run it
raw_insult <- req_perform(req)
#check the result
raw_insult
names(raw_insult)
str(raw_insult)
raw_insult$body

#wow that body looks weird. we have to decode it
my_insult <- resp_body_json(raw_insult) 
str(my_insult)

#convert to dataframe
data.frame(my_insult)

```

### Why do we want the JSON result instead of text?
You can test it out, but the text result only provides the quote and none of the attribution or stats.
We want it all!

# Challenge!
 - Append your insult to the "UDA Insults" google sheet.  
 - Check the sheet ahead of time to see what structure your insult should be in.  
 - Done early? Generate a few more insults (try different languages, perhaps!) and add them also
```{r}

```


## Simple Scraping Example

### Pull in a table of data by position
Let's grab the consumer price index
```{r}
#we need packages and pipes
library(rvest)  #rvest is a packaged designed for scraping
library(magrittr)

#we need a link to scrape from 
cpiLink <- "https://www.usinflationcalculator.com/inflation/consumer-price-index-and-annual-percent-changes-from-1913-to-2008/"

#we need to pull what we want from the HTML
#first we read in the code from the link
cpiTable <- read_html(cpiLink) %>% 
  #then we pull in the table elements as a list
  html_table(header = TRUE) %>% 
  #then we select the particular list item we want, and return it as a tibble.  
  #The (1) is the list index
  `[[`(1)

#take a look
rbind(head(cpiTable, 3), tail(cpiTable, 3))

#can you clean it up so the first row becomes column names?
```

### Pull in a table of data by HTML information

When we use the **html_table** function, it gets a list of every table on the page, whether it is 1 or 100 tables. The extraction is great when you know which table you want and the indexed order of it will never change. However, you might run into the case where it does change. 

Let's see what we might be able to do to program around that.

```{r}
#same startup
topFilms <- "https://en.wikipedia.org/wiki/List_of_highest-grossing_films"

topFilmsRead <- read_html(topFilms)


table_number <- 
  topFilmsRead %>% 
  
  #we have to look within the HTML to find the bits that are tables
  html_nodes("table") %>%
  #and then within those bits, we are looking for this particular character string 
  #(TRUE if found, FALSE if not)
  grepl(">Highest-grossing films<", .) %>%
  # Then we ask which table index value is TRUE 
  which(. == TRUE)

# And we use that value, as before, to pull in the table datsa.  WHEW.
topFilmsRead %>% 
  html_table() %>% 
  `[[`(table_number)
```

Nodes and pattern searches can get much more complicated, but require knowledge of html tags --- we don't need to go that far here. 
