---
html_document:
  toc: yes
author: "Unstructured Data Analytics, Fall 2024"
output:
  html_document:
    df_print: paged
always_allow_html: yes
title: 'Analyzing Text: Advanced Counting'
toc_float: yes
theme: spacelab
highlight: tango
df_print: paged
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


# Advanced Counting
```{r echo=TRUE, message=FALSE, warning=FALSE}
# load initial packages
library(tidyverse)
library(tidytext)
library(googledrive)
library(googlesheets4)
```

```{r}
# read it in and save - free code!
drive_auth()
gs4_auth(token = drive_token())
ss <- drive_get("UDA 2024 South Bend")
dat <- read_sheet(ss)

head(dat)
```

### Review -- basic startup
Tokenize by word, remove stopwords (any/all), count  

Save results in a dataframe for more use later.
```{r}
sb_counts <- unnest_tokens(dat, 
              output = "word",
              input = "SB_Thoughts",
              token = "words") %>%
  anti_join(stop_words, by = "word") %>%
  count(word, sort = T)

head(sb_counts)
glimpse(sb_counts)
```


### Visuals

#### Wordcloud (from the end of last class)

```{r}
#load new package [install as needed]
library(wordcloud)
```


Take a look at the help file for the wordcloud function.  

```{r echo = FALSE, eval = FALSE}
#view help
help(wordcloud)
```
  
You'll need to use a **minimum** of two arguments: 

1. the dataframe column that has your words for the "words" argument (use $ notation in the function)  
2. the dataframe column that has your counts for the "freq" argument (use $ notation in the function)  
  
After you get those two arguments working, look at the help page and try out **two MORE optional arguments**. These arguments will (in some way) change the appearance of your plots.  
  
Note that the 2D position of the words is random by default.  
If you want a *consistent* placement, set a seed before you build the wordcloud  
(And if you don't like the picture it makes, try different seeds)  
```{r}
#your wordcloud here
wordcloud(words = sb_counts$word,
          freq = sb_counts$n,
          max.words = 50,
          min.freq = 6,
          random.order = FALSE,
          rot.per = 0.4,
          random.color = FALSE,
          colors = c("darkgreen", "pink", "navy", "gold"))
```


#### ggplot for count data  
  
Note: the behavior of your data below will depend a little bit on how you created your counts above. If you use `group_by`, your data will retain a grouped structure by default.  If you use `count`, your data will be treated like a standard dataframe instead of structurally grouped.  
  
**Plot it!**

*If you aren't feeling confident, start with one element and then build up your code piece by piece*
  
 * Can you create a bar chart that shows words and their wordcounts?  
 * Can you limit your bar chart to show only the top (x) words? (Pick a value for X)
 * Can you customize the title?
 * Can you get the words on the y-axis and the counts on the x-axis?
 * *Challenge:* Can you automatically sort the bars from most-to-fewest? This needs to happen within a *mutate* using the `reorder` function (look it up!) 


```{r}
sb_counts %>%
  head(15) %>%
  ggplot(aes(x = reorder(word, n), 
             y = n)) +
  geom_bar(stat = "identity") +
  xlab("word") +
  ylab("count") +
  coord_flip() +
  ggtitle("Top words from class writing about South Bend") +
  theme_minimal()

```
    
This is pretty simple but does add a nice visual piece. To make it even better, I could try to put the words onto the bars and label the bar-ends with counts. Then I wouldn’t even need the axes at all, for a much cleaner graph. For a simple barchart like this, it’s also fine to include more words — how many depends (to some extent) on the space you have available. 



# Advanced Counting
Starting with our same original data, let's go beyond basic word frequencies.


## Can we look at phrases instead of words?
Of course we can! We tokenize differently, but the other steps are the same.  
Note that *phrases* are repeated less often than *individual words* so you typically need a lot of data (or very consistent data) to get good results. 

Let's tokenize into phrases and then count
```{r}
unnest_tokens(dat, 
              output = "bigram",
              input = "SB_Thoughts",
              token = "ngrams",
              n = 2) %>%
  count(bigram, sort = T)

```

Vocab alert: We use Greek prefixes to give small ngrams "names".   
  
 * A word is a unigram
 * A 2 word phrase is a bigram
 * A 3 word phrase is a trigram
  
### What about stopwords for phrases?
You may have noticed some less useful ngrams above, for example, ones that are 100% stopwords.   
If we’re looking at straight counts, we might want to remove them. If we’re going to end up doing TF-IDF (coming soon), we don’t have to.  
   
An “ngram stopword lexicon” doesn't exist (yet?), so the process is a little clunky.  
  
First, I am going to save my stopwords out in a separate vector.  
```{r}
# SKIP
```


Then, to remove stopword bigrams, we have to find the bigrams that are composed of 2 stopwords.  
  
One way to do this is: break the bigram words into 2 columns, check against stopwords, remove rows with both 
```{r}
unnest_tokens(dat, 
              output = "bigram",
              input = "SB_Thoughts",
              token = "ngrams",
              n = 2) %>%
  separate(col = "bigram", 
           into = c("w1", "w2"), 
           sep = " ", 
           remove = FALSE) %>%
  #this will remove ANY bigram with a stopword
  filter(!(w1 %in% stop_words$word),
         !(w2 %in% stop_words$word)) %>%
  count(bigram, sort = T) %>%
  head(12)
```
I like that the idea of a "bubble" shows up here, which we might not otherwise notice. (Bubble itself is much less common than other single words)

```{r}
unnest_tokens(dat, 
              output = "bigram",
              input = "SB_Thoughts",
              token = "ngrams",
              n = 2) %>%
  separate(col = "bigram", 
           into = c("w1", "w2"), 
           sep = " ", 
           remove = FALSE) %>%
  #this will only remove bigram if BOTH words are stopwords
  filter(!(w1 %in% stop_words$word & w2 %in% stop_words$word)) %>%
  count(bigram, sort = T) %>%
  head(12)
```


Now you can see common multi-word phrases.  Sometimes more informative than single words, because we're getting a little more context!


## Scaling counts

**These techniques work for single words and for n-grams.**
For simplicity, we'll do them with single words.

To compare texts to each other (not together in a lump), frequencies aren't really fair.  
Why?  Longer texts will have more words.   

We can work around this using a few methods that scale our results.  

Start with your original wordcounts from above, all together.

  - Can you add a column with the total wordcount for the corpus?
  - Can you compute the percentage of the corpus that each word supplies? (another column)
  - Can you get this on a more "human" scale? (frequency per 1000 or per 10000) (another column!)
  
Save your results as `total_counts`
```{r}
total_counts <- sb_counts %>%
  mutate(totalwords = sum(n),
         word_proportion = n/totalwords,
         word_rate_10000 = round(word_proportion*10000))

head(total_counts)

```


  
Notice that for only one document, scaling the numbers doesn’t provide much new information. It’s just another way to look at simple counts.  
This process gets more useful when we compare documents of different lengths. It’s a way to normalize.  
  
### Let's scale counts by group
Your choice of group: by feeling *or* by section *or* by person (row)

 * copy and adjust the code from above
 * save the result as `group_counts` instead of `total_counts`

```{r}
# make new counts
group_counts <- unnest_tokens(dat, 
              output = "word", 
              input = "SB_Thoughts", 
              token = "words") %>%
  anti_join(stop_words, by = "word") %>%
  
  #now, we're doing counts and scaling section-by-section instead of all together
  group_by(Section, word) %>%
  summarize(n = n()) %>%
  mutate(totalwords = sum(n),
         word_proportion = n/totalwords,
         word_rate_10000 = round(word_proportion*10000)) %>%
  arrange(Section, word_rate_10000) %>%
  slice_max(order_by=word_rate_10000, n = 7)

group_counts

```  

  
### Check relative rates -- two ways to write the same thing
For example: which section/feeling/person uses "campus" the most, relative to the other groups?  
**Notice: for this example, the N values are inverted from the scaled values**
```{r}
#dplyr version
#you do have to write a little regex, but I think this code is worth it
#it's so much easier to read than the base
group_counts %>%
  arrange(desc(word_rate_10000)) %>%
  filter(str_detect(word, "^campus$")) 


#base R version, if you're curious (might be a better way, I'm a tidy girl...):
one_word <- group_counts[group_counts$word == "campus", ] 
one_word[order(one_word$word_rate_10000, decreasing = TRUE), ]
```
  
Try your own: Copy and paste the chunk from above and try a different (more interesting?) word  
You can reset the data to slice_max a different number (or not slice_max at all) to dig farther down

```{r}
#dplyr version, your word

```
  
# TF-IDF (not in class, see Tuesday prep)


## How "important" is a particular word to a particular document?
**Importance** can often be measured by **count** or **frequency**, but sometimes important words aren't the most common, but the most unique. This is particularly true if we're trying to understand what makes some documents *different* from each other.

For example, if you read a "how to play soccer" and "how to play lacrosse" guide, they are both going to talk about balls, fields, goals, players, lines, shots, passing, fouls, etc.  But there may be specific vocabulary that appears more (or more often) in one document than in the other. For example, lacrosse might talk about the "cradle"; soccer might talk about "headers": these would be potentially very relevant, even if they don't show up very often.

#### How do get to an idea of uniqueness or importance? 
We already have frequency measures (count/total). We can add a measure for the proportion of documents that contain a particular word.  
  
For example: How many documents use "restaurant?" How many documents use "campus"? How may documents use "the"? (Out of the total number of documents).  
  
Let's calculate this and add it to our `group_counts` dataframe
```{r}
# now with document frequencies
# first we need to know how many total documents
# when looking at the data by a category, each "combined set of words from that category"
# counts as a document

  
```

  
### Now we get fancy: TF-IDF
We want to take our "term percent" concept and our "document ratio" concept and combine them to normalize the data.
Check the slide deck for a full explanation of the math.

TF-IDF = Term Frequency - Inverse Document Frequency  

Intuition: Important words are used a lot within a document, and are rarely used in other documents.  

 * "used with a document" is the count, weighted by total wordcount (term frequency)
 * "used in other documents" is the document frequency we just calculated
 * to get "RARELY used in other documents", we take the inverse document frequency 
 * because IDF is usually much larger than TF, we take the log of IDF as a way to re-scale it

Formula: term_frequency * log(inverse(document_frequency))

```{r}
#let's calculate TF-IDF "by hand", then sort large > small

```

  
### Do we have to do TF-IDF by hand? Of course not!
Use the tidytext package function instead
```{r}
#you do have to pre-count your data for this to work


```
Notice that the “tf” computed by the package function can be multiplied by 10000 again to get our word rate. IDF isn’t usually interesting on its own. The TF-IDF values here should match the manual calculations above.  
  
  
### Ways to adjust TF-IDF to solve potential problems
Sometimes TF-IDF is a little too constrained. 
```{r}
# Want to implement smoothing? (extra idea, see slides)
# In that case, we have to compute TF-IDF manually so we can adjust the calculation, for example:


```
We do have stopwords sneaking back in here, but not as many as before. Smoothing may be more effective with a larger number of documents or with longer texts.  
  

# Bonus Time

## Can you do TF-IDF with ngrams?
Sure! And we don’t need to remove stopwords either.  
It’s the same code as before, grouping, tokenizing, counting, TF-IDF.  
The only difference is how we tokenize, and the related column names.  

Try it out!
```{r}

```

  
## Visualizing
For any of the things done here, it’s simple to take your final dataframe and create visualizations out of it. You can create wordclouds, bar graphs, or faceted bar graphs just like you did with simple counts. The only different in plotting is that you’ll have to use the correct column names for the data you want to look it. For that reason, no plotting code is included here. You can copy and adjust the code from the previous day's code file.
```{r}

```

