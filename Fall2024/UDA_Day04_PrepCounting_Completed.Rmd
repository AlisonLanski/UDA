---
html_document:
  toc: yes
author: "Unstructured Data Analytics, Fall 2024"
output:
  html_document:
    df_print: paged
always_allow_html: yes
title: 'Analyzing Text: Preparations & Counting'
toc_float: yes
theme: spacelab
highlight: tango
df_print: paged
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Well well well, What do we have here? 
*First explorations of a new dataset*

```{r echo=TRUE, message=FALSE, warning=FALSE}
#load initial packages
library(tidyverse)
```

  
## Let's read in some data and check it out

```{r}
library(googledrive)
library(googlesheets4)
```
Auth problems? You can skip this part -- and some of the next code
*If you're stuck? You can open the actual google sheet in Drive, download a csv from it, and read that in below instead*
```{r}
drive_auth()
gs4_auth(token = drive_token())
```

```{r}
drive_find("UDA 2024", type = "spreadsheet", n_max = 10)
```

Read in the google sheets data.
```{r}
ss <- drive_get("UDA 2024 Insults")
df_raw <- read_sheet(ss, range = "Sheet1!C:E", col_types = "c")
```

Minor data cleanup
```{r}
df <- df_raw %>% 
  filter(!is.na(Insult), nchar(Insult) > 3) %>% 
  group_by(Insult) %>% 
  summarize(CreatedBy = max(CreatedBy, na.rm = T),
                           TimesShown = max(TimesShown, na.rm = T)) %>%
  filter(str_detect(Insult, pattern = "^[:alpha:]"))
```


#### Who has created the most distinct insults?  
```{r}
df %>% 
  group_by(CreatedBy) %>% 
  summarize(InsultCount = n()) %>%
  arrange(-InsultCount)
```


#### What are those "top creator" insults?
```{r}
df %>% filter(CreatedBy == "Martin Luther")
```


## "Bag of words" methods

*How do we find the words?*
*What issues might we run into?*

### Dividing and Counting
Can you count how many words there are in one Insult? In all the insults?  
Is there a typical length of an insult?
Are there particular words that might be relevant to or associated with insults?


How can we find individual words in an insult?
```{r}
#total number of words
str_count(df[1,1], pattern = " ")+1
```

```{r}
#what is each individual word?  -- split it out, breaking at spaces
str_split(df[1,1], pattern = " ") %>% unlist() %>% tolower()
```

```{r}
# another way: split it out by grabbing the "word stuff" and ignoring the rest
insult1 <- str_extract_all(string = df[1,1], pattern = "\\w+") %>% 
  unlist() %>%
  tolower()

insult1
```


```{r}
#let's count -- how many times each word appears
table(insult1)
```

### Is there an easier way?
Yes, of course, this is R.
```{r}
#load package
library(tidytext)
```
Divide and prep
```{r}
#use package
df_insults <- df %>% 
  unnest_tokens(output = "word", input = "Insult", token = "words", drop = FALSE)
  # output = columnname we want to create and put our words in
  # input = column name to find the text we want to tokenize
  # tokenize: what level to tokenize at
  # drop: should we also keep the original text all together in a single string?

head(df_insults)
```
What changes were made from the starting text to the version? 
```{r}
#write code to explore as needed

## compare to above
```

#### Do it again with new data
Let's look at some news articles about the election
**As before, download as csv and read in if google drive is not working for you**
```{r}
#Get the other data
ss2 <- drive_get("UDA 2024 Election News")
df_raw <- read_sheet(ss2)
```

Divide & prep
```{r}
#articles 
election_articles <- df_raw %>% 
  unnest_tokens(output = "word", 
                input = "Article",
                token = "words")

head(election_articles)
```

```{r}
#headlines
election_headlines <- df_raw %>% 
  unnest_tokens(output = "word",
                input = "Headline",
                token = "words",
                drop = FALSE)

head(election_headlines)
```

#### Can we combine our data sources?
We still want to know which words are from which sources
```{r}
 #skip for now!
```


## Counts are surprisingly useful

### Which words are most common overall? 
### Which words are most common in each data source? 

Can you create a top-five or top_ten list of words for the whole file? 
Can you create a top-five or top_ten list of words for each source?
*Store these as new objects called "top_by_file" and "top_by_doc"*

```{r}
#everyone loves a "top" list
election_articles %>% 
  count(word, sort = TRUE) %>% 
  head(10)
```


```{r}
#and another! -- same idea, different code format
election_headlines %>% 
  group_by(word) %>% 
  summarize(wordcount = n()) %>%
  arrange(-wordcount) %>%
  head(10)
```

## Can we get a better list of common words?
Yes, of course. 

```{r}
#load vocabulary lists
stop_words
```
How many lexicons are there? 
How many words are there in each lexicon?
*Do you want more dictionary options? Try the "stopwords" package.  It has additional stop words dictionaries, including support for foreign languages.*
```{r}
#explore lexicons
stop_words %>% count(lexicon)
```

What happens to our counts if we remove the stopwords using the SMART lexicon?
```{r}
#remove SMART words
# use base R to limit the stopwords
election_articles %>% 
  count(word, sort = TRUE) %>%
  anti_join(stop_words[stop_words$lexicon == "SMART", ], 
            by = "word")
```

```{r}
# use tidyverse to limit the stopwords
election_headlines %>% 
  group_by(word) %>% 
  summarize(wordcount = n()) %>%
  arrange(-wordcount) %>%
  anti_join(stop_words %>% filter(lexicon == "SMART"), by="word") %>%
  head(10)
```

Can we see which stopwords were removed?  
(Are there any we might want to keep?)
```{r}
#review what we lost

#(we can do this with an inner join)
inner_join(election_headlines, 
           stop_words[stop_words$lexicon == "SMART",], 
           by = "word")
```

If we look at top 5/top 10 again -- is this more meaningful? 
```{r}
election_articles %>% 
  count(word, sort = TRUE) %>%
  anti_join(stop_words[stop_words$lexicon == "SMART", ], 
            by = "word") %>%
  head(10)
```

```{r}
#top_n needs a groupby to work correctly, 
#but will then extract the top words in each group instead of overall
election_headlines %>% 
  group_by(Publication, word) %>%
  summarize(wordcount = n()) %>%
  arrange(Publication, -wordcount) %>%
  anti_join(stop_words[stop_words$lexicon == "SMART", ], 
            by = "word") %>%
  top_n(n = 5, wt = wordcount)
```

# Challenge Task 1: Create a wordcloud to visualize your data
## Everyone loves a good wordcloud

Yes, you can do this manually in ggplot by setting different random x and y values for each word and then replacing a scatterplot point with the word itself but... that's a lot of work.

Instead. use this new-to-you package
```{r echo = FALSE, eval = FALSE}
# you'll probably need to install this package
# this chunk of code will be ignored when you knit 
install.packages("wordcloud")
```

```{r}
#load new package
library(wordcloud)
```


Take a look at the help file for the wordcloud function.  

```{r echo = FALSE, eval = FALSE}
#view help
help(wordcloud)
```


  
  
You'll need to use a minimum of two arguments: 

1. the dataframe column that has your words for the "words" argument (use $ notation in the function)  
2. the dataframe column that has your counts for the "freq" argument (use $ notation in the function)  
  
After you get those two arguments working, look at the help page and try out two MORE optional arguments. These arguments will (in some way) change the appearance of your plots.  
  
  
*Use either the insults or the south bend part of the data*  
  
Note that the 2D position of the words is random by default.  
If you want a consistent placement, set a seed before you build the wordcloud  
(And if you don't like the picture it makes, try different seeds)  
```{r}
#your wordcloud here
election_50 <- election_articles %>% 
  count(word, sort = TRUE) %>%
  anti_join(stop_words[stop_words$lexicon == "SMART", ], 
            by = "word") %>%
  head(50)

#your wordcloud here
wordcloud(election_50$word, election_50$n)
```

This wordcloud uses mandatory arguments for the words and the wordcounts, then uses optional arguments to try to improve the overall look. There are many other optional arguments that can be used instead of these (or in addition) which you can see from the help page.  
  

    
#### Put on your Data Viz hat. How did your two "optional arguments" affect the preattentive qualities of your visualization? Are your updates better than the default arguments? Why/why not?

  
  
# Challenge Task 2: ggplot to visualize count data  
  
Note: the behavior of your data below will depend a little bit on how you created your counts above. If you use `group_by`, your data will retain a grouped structure by default.  If you use `count`, your data will be treated like a standard dataframe instead of structurally grouped.  
  
## Plot it!

*Start with either insults or articles (counted) and build up your code piece by piece*
  
 * Can you create a bar chart that shows words and their wordcounts?  
 * Can you limit your bar chart to show only the top (x) words? (Pick a value for X)
 * Can you customize the title?
 * *Challenge-er:* Can you flip your axes so the words are on the y-axis and the counts are on the x-axis? (look it up!)
 * *Challenge-est:* Can you automatically sort the bars from most-to-fewest? This needs to happen within a *mutate* using the `reorder` function (look it up!) 


```{r}


```
    
This is pretty simple but does add a nice visual piece. To make it even better, I could try to put the words onto the bars and label the bar-ends with counts. Then I wouldn’t even need the axes at all, for a much cleaner graph. For a simple barchart like this, it’s also fine to include more words — how many depends (to some extent) on the space you have available. 

*Can you do something similar to what you did above, but keep all the data (south bend & insults) and facet the graph to have separate bars for each topic?*

```{r}



```

