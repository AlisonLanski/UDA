---
html_document:
  toc: yes
author: "Unstructured Data Analytics, Fall 2023"
output:
  html_document:
    df_print: paged
always_allow_html: yes
title: 'Analyzing Text: Sentiment Analysis'
toc_float: yes
theme: spacelab
highlight: tango
df_print: paged
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# How can we detect if contain positive/negative feeling or specific emotion?




## First Step: Analyze Polarity with Lexicons
```{r echo=TRUE, message=FALSE, warning=FALSE}
# load initial packages
library(tidyverse)
library(tidytext)
```

First step: getting lexicons  
*For some, you have to download them: switch to the console to approve download*
```{r}
bing <- get_sentiments(lexicon = "bing")
afinn <- get_sentiments(lexicon = "afinn")
nrc <- get_sentiments(lexicon = "nrc")
```

  
### *Question: How do these sentiment lexicons vary in size and structure?*   
Let's take a look
```{r}
glimpse(bing)
glimpse(afinn)
glimpse(nrc)
```

  
## Let's keep our first focus on polarity

### Question: If we want to use the afin data to score our stories, what general process should we use?

### Question: If we want to use the bing data to score our stories, what is different?


## Implementation
First, we have to get our text data ready

```{r}

#read in
dat <- read_csv("C:/Users/alanski/Downloads/UDA Fall 2023 Stories v3.csv")

#explore
glimpse(dat)

```

 * What columns do we have?
 * Do we need to fix the sport column again?
 * Do you think we need to remove stopwords? 
 
 
Cleanup, as needed.  Prep appropriately for our next step
```{r}
dat$sport <- tolower(dat$sport)

dat_words <- dat %>% 
  unnest_tokens(output = word, input = story, token = "words")

dat_words
```
Combine words with sentiment: afinn
```{r}
dat_words %>%
  inner_join(afinn, by = "word")
```

### Question: What is the polarity of each story?

```{r}
dat_words %>%
  inner_join(afinn, by = "word") %>%
  group_by(id, feeling) %>%
  summarize(total_sentiment = sum(value),
            avg_sentiment = mean(value))
```
### What do the polarity numbers represent? 

```{r}
dat_words %>%
  inner_join(bing, by = "word") %>%
  group_by(id, sentiment) %>%
  count() %>%
  pivot_wider(names_from =  sentiment, values_from = n, values_fill = 0) %>%
  mutate(percent_positive = round(positive/(positive+negative),3))
```


```{r} 
dat_words %>%
  inner_join(nrc, by = "word") %>%
  group_by(id, feeling, sentiment) %>%
  count() %>%
  pivot_wider(names_from =  sentiment, values_from = n, values_fill = 0) %>%
  mutate(total_score = positive - negative,
         percent_positive = round(positive/(positive+negative),3)) %>%
  select(id, feeling, positive, negative, total_score, percent_positive)
```

## How might we graph polarity?
```{r}
dat_words %>%
  inner_join(afinn, by = "word") %>%
  group_by(id, feeling) %>%
  summarize(total_sentiment = sum(value),
            avg_sentiment = mean(value)) %>%
  mutate(polarity = ifelse(total_sentiment > 0, "positive", "negative")) %>%
  ggplot(
    aes(x = id, y = total_sentiment, fill = polarity)
  ) +
  geom_col()
```


## Which emotions are present in each story?
### Which emotion is the most prevalent? What is the mix like? 
```{r}
dat_words %>%
  inner_join(nrc, by = "word") %>%
  group_by(id, feeling, sentiment) %>%
  count() %>%
  filter(!sentiment %in% c("positive", "negative")) %>%
  group_by(id) %>%
  slice_max(order_by = n, n = 1)
```


```{r}
dat_words %>%
  inner_join(nrc, by = "word") %>%
  group_by(id, feeling, sentiment) %>%
  count() %>%
  pivot_wider(names_from =  sentiment, values_from = n, values_fill = 0)
```


### Let's visualize the mix of emotions within a document
```{r}
library(RColorBrewer)
dat_words %>%
  inner_join(nrc, by = "word") %>%
  group_by(id, feeling, sentiment) %>%
  count() %>% 
  filter(!sentiment %in% c("negative", "positive")) %>%
  ggplot(
    aes(x = id, y = n, fill = sentiment)
  ) +
  geom_col(position = "fill") +
  scale_fill_brewer(palette = "Set1") +
  coord_flip() +
  theme_minimal() +
  ggtitle("Sentiment range for each story")

```

Or can focus on the absolute count of sentiment words for one or more emotion
```{r}
library(RColorBrewer)

dat_words %>%
  inner_join(nrc, by = "word") %>%
  group_by(id, feeling, sentiment) %>%
  count() %>% 
  filter(!sentiment %in% c("negative", "positive")) %>%
  
  #add a specific set of emotions if you want to see them
  filter(sentiment %in% c("joy", "sadness")) %>%
  
  ggplot(
    aes(x = id, y = n, fill = sentiment)
  ) +
  geom_col() +
  scale_fill_brewer(palette = "Set1") +
  coord_flip() +
  theme_minimal() +
  ggtitle("Sentiment range for each story") #+
  #facet_wrap(~sentiment)  #uncomment this bit to break the bars out by emotion
```





# What about context: issues like valence?
## Can we view sentiment at a higher level, beyond single words?

Yes, of course we can!  

This capability lets us account (to some extent) for valence shifters.

### Implementing higher-level sentiment
The tidytext package is set up for word-level only.  That means we have to shift to the `sentimentr` package.  
  
Let's find new sentiment values for each story, taking valence into consideration
```{r}
library(sentimentr)

story_polarity <- get_sentences(dat) %>%
  sentiment() %>%
  group_by(id) %>%
  summarize(total = sum(sentiment),
            high = max(sentiment),
            low = min(sentiment),
            avg = mean(sentiment))

story_polarity
```
How can we easily check for example sentences or review the scoring more visually?
```{r}
dat %>%
  get_sentences() %>%
  sentiment_by(by = c("sport", "id")) %>%
  highlight()
```


## Sentiment over time, sample code with lubridate and/or more sentimentr features/graphing
See sentiment trend
```{r}
dat %>%
  get_sentences() %>%
  sentiment() %>%
  ggplot(
    aes(x = sentence_id, y = sentiment)
  ) +
  geom_line() +
  facet_wrap(~id) +
  theme_void() +
  geom_hline(aes(yintercept = 0), color = "grey")





```

#messing around with dates
```{r}

#grab a student submission from Assignment #1 as an example

reddit <-  read_csv(file = paste0("C:/Users/alanski/Downloads/scrape/ahernmadison_9235_2821518_reddit_posts-2.csv"))

reddit_dates <- reddit %>% select(date_utc, title) %>% unnest_tokens(output = word, input = title, token = "words")


reddit_dates %>%
  mutate(date_monthno = lubridate::month(date_utc),
         date_weekno = lubridate::week(date_utc),
         date_week = lubridate::floor_date(date_utc, unit = "week"),
         date_month = lubridate::floor_date(date_utc, unit = "month"),
         date_days = lubridate::wday(date_utc)) %>%
  inner_join(afinn) %>%
  group_by(date_month) %>%
  summarize(feeling = mean(value, na.rm = TRUE)) %>%
  ggplot(
    aes(x = date_month, y = feeling)
  ) + 
  geom_bar(stat = "identity")

reddit_times <- reddit %>% select(date_utc, timestamp, title) %>% unnest_tokens(output = word, input = title, token = "words")

reddit_times %>%
  mutate(dt = lubridate::as_datetime(timestamp),
         dt2 = substr(dt, 12, 19),
         clock = hms(dt2),
         hrs = lubridate::hour(dt),
         mins = lubridate::minute(dt),
         secs = lubridate::second(dt))
```

