---
html_document:
  toc: yes
author: "Unstructured Data Analytics, Fall 2023"
output:
  html_document:
    df_print: paged
always_allow_html: yes
title: 'Text Analytics: Topic Models'
toc_float: yes
theme: spacelab
highlight: tango
df_print: paged
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
# load initial packages
library(tidyverse)
library(tidytext)
```


# Can we detect topics in a corpus and categorize documents appropriately? Which key words contribute to each topic?

## Get set up
```{r}
# Read in class data
dat <- read_csv("UDA Fall 2023 Stories v3.csv") %>%
  mutate(sport = tolower(sport))

# Perform basic prep
dat_words <- dat %>%
  unnest_tokens(output = word, input = story, token = "words") %>%
  anti_join(get_stopwords(source = "smart")) %>% 
  filter(str_detect(word, pattern = "[:alpha:]")) %>%
  count(id, word) 
```

## Prep data for topic modeling
We need a document term matrix
```{r}
library(topicmodels)
```

### QUESTION: What would we expect about the matrix if we switched to ngrams? 

## Run topic models
We have important parameters here.  
  
**alpha** dictates how the topics are distributed across the documents

 * As alpha increases, documents are more likely have a mix of topics
 * As alpha decreases, documents are more likely to have a single topic
 * A common default is 50/# of topics (when you have a large corpus)
 
**delta** dictates how the words are distributed across the topics

 * As delta increases, topics will have more associated words
 * As delta decreases, topics will have fewer associated words (and words are more likely to be uniquely associated with a topic) 
 * A common default is 0.1
   
Other parameters we can tune include  

 * **iter** the number of iterations (default 2000)
 * **burnin** how many early iterations to discard (default 0)
```{r}
#let's create a 2-topic model

```

```{r}
#pick a higher number (3-5) and try again


```

## Can we find words strongly associated with particular topics?
This is beta. For each topic, beta adds up to 1.
```{r}
# start with the 2-topic model

```

```{r}
# same code, but try it on your 3-topic model

```

### Visualize? Can use the old standard
Faceted barplot
```{r}
??your_data_here?? %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(mapping = aes(x = beta, y = term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```


## Can we find documents strongly associated with specific topics?
This is gamma. For each document, gamma adds up to 1.
```{r}
#gamma for the 2-topic model, find the max values per topic

```
```{r}
#same thing with gamma for the other model

```


### Free Viz! Another way to look at words that most strongly associated with one of two topics
```{r}
#distance between topic-related words
#(shows most single-topic-y words)

corpus_lda2 %>% 
  tidy(matrix = "beta") %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1)) %>%
  mutate(log_type = ifelse(log_ratio > 0, "Topic 2 Association", "Topic 1 Association"),
         abs_log = abs(log_ratio)) %>%
  group_by(log_type) %>%
  slice_max(order_by = abs_log, n = 8) %>%
  mutate(term = reorder_within(x = term, by = log_ratio, within = log_type)) %>%
  ggplot(
    aes(x = log_ratio, y = term, fill = log_type)
  ) +
  geom_col()+
  scale_y_reordered()
```

## Are results different when we allow for correlation instead of instead of LDA independence?
Because CTM uses a logistic normal distributions instead of a latent dirichlet allocation....  

 * we no longer have alpha and delta parameters
 * we still have beta and gamma outputs
```{r}



```

#### What topics emerge? How should we label them? How evenly is our corpus split into the topics?

### Which model do you like the best and why?