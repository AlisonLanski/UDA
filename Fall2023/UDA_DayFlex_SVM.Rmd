---
html_document:
  toc: yes
author: "UDA MSSA Fall 2023"
output:
  html_document:
    df_print: paged
always_allow_html: yes
title: 'Classification via Support Vector Machines'
toc_float: yes
theme: spacelab
highlight: tango
df_print: paged
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# How can I classify this new text into existing categories?

### Prep and reshape the data
Load packages
```{r, warning= FALSE, message=FALSE}
library(tidyverse)
library(tidytext)
```

Our class data is too small and inconsistent for this to work well, so let's try a larger reddit dataset
```{r}
dat <- read_csv("???/reddit_three.csv")

head(dat)
```

SVM can work well with sparse data.  It can work well with (relatively) small data.  (Hundreds instead of thousands of observations).  It can also work well with a large number of features, but can perform better if you limit the feature space.  

Ideally, we want to have more observations than features.  Because this total dataset has over 5000 words (features), we'll limit them here by only keeping "frequent" words, but removing the very-frequent stopwords and non-informative stopwords.  
```{r}
#find the set of words that is "frequent"
common <-  dat %>%
  mutate(full_text = paste(title, text)) %>%
  unnest_tokens(word, full_text) %>%
  count(word) %>%
  filter(n > 4)

# prep the main data, make it a little smaller, reshape
dat_wide <- dat %>%
  mutate(idlabel = row_number(),
         full_text = paste(title, text)) %>%
  unnest_tokens(output = word, input = full_text, token = "words") %>%
  filter(str_detect(word, pattern = "[:alpha:]")) %>%
  group_by(idlabel, sport, word) %>%
  summarize(count = n()) %>%
  ungroup() %>%
 #for the svm model to work, a categorical label must be a factor
  mutate(ml_label = factor(sport)) %>%
  anti_join(stop_words, by = "word") %>%
  inner_join(common, by = "word") %>%
 #remove columns we don't need pre-pivot
   select(-sport, -n) %>% 
   pivot_wider(names_from = word, values_from = count, values_fill = 0) %>%
  #convert to one-hot encoding
   mutate(across(3:ncol(.), ~ifelse(. >= 1, 1, 0)))

head(dat_wide)  
```
Looks good.  By the second page, I can already see specific sport-related vocabulary (spirit, audl)

### Create train and test sets
The `caret` library is a nice wrapper for other ML libraries and functions. 
```{r}
library(caret)

#create 80/20 train/test split
set.seed(1234)
sample_set <- createDataPartition(y = dat_wide$ml_label, p = .8, list = FALSE)
dat_train <- dat_wide[sample_set, ]
dat_test <- dat_wide[-sample_set, ]

#check that the data has split somewhat evenly by category
round(prop.table(table(select(dat_wide, ml_label))),2)
round(prop.table(table(select(dat_train, ml_label))),2)
round(prop.table(table(select(dat_test, ml_label))),2)
```
Ratios look good - consistent across labels.


## Run the SVM model
The svm functions are part of the `kernlab` package, although we are going to access them via caret.
svm can handle multiple categories. The function will automatically try to scale the data.  If we wanted to pre-scale text data, we could compute term-frequencies instead of just counts, so all values would be between 0 and 1.    
Two key choices when running an SVM model are the type of kernel (the `method` argument) and the hardness/softness of the boundaries: how far away from the border an items can be misclassified (the `C` argument in `tuneGrid`)
```{r}
#load the library for svm
library(kernlab)

#create the model: train on everything except the ID and label columns
#we're using 5-fold crossvalidation and a Cost of 1

set.seed(1234)
svm_mod <- train(
  ml_label ~ . -idlabel,
  data = dat_train,
  method = "svmLinear",
  trControl = trainControl(method = "cv", number = 5),
  tuneGrid = expand.grid(C = c(0.01, 0.1, 1, 10, 100)))

svm_mod

```
When we try multiple values of C instead of only a single value, the model will select the best result by default and report it in the model output.  If we like these results, we can proceed to checking it against our test set. If we don't like it, we should do more tuning before trying it against our test data.  

#### Note
If, when running the model, you get warning messages like this:  
*Warning: Variable(s) `' constant. Cannot scale data.*    
They are being generated because some columns have the same value in every row.  Our data only experiences this problem in the cross-validation folds (when the data has fewer rows).  If our starting data had columns like this, we should remove them because they aren't useful for prediction.  
  
### Prediction happens like it normally would  
```{r}
svm_pred <- predict(svm_mod, dat_test)
```

See a sample of results
```{r}
head(svm_pred)
```

### Evaluate results
As typical for classification problems, we can view a confusion matrix to check the accuracy of predictions
```{r}
#caret has a nice function to produce a confusion matrix for us with stats
svm_matrix <- confusionMatrix(svm_pred, dat_test$ml_label, positive = "ultimate")

#view all results
svm_matrix
```
What's important in these results?  

 * Accuracy: the % of correct predictions. Nice to be above 0.7
 * Kappa is a weighted accuracy that accounts for the fact that high accuracy can happen simply by chance. A strong result is above 0.75, above 0.4 is considered moderately accurate. Values range from -1 to 1, so negative numbers aren't good!
 * Mcnemar's P-Value has a null hypothesis that predictions are inconsistent (more or less), so having a value > 0.05 means we can reject the null, which is good for model validity.
 * For the individual classes, sensitivity (true positive rate) and specificity (true negative rate) are good ones to consider. Remember that these often vary inversely (as one goes up, the other goes down)
  

The actual results for our model are good! With C = 1 and without converting to one-hot, they are less good (you can adjust and run the code if you want to see it, but still ok)    

### Just look at the validation elements you care about
Pull out the confusion matrix
```{r}
svm_matrix$table
```

Pull out the model stats.  The `tidy` function from the package `broom` gives us a dataframe for the result set.
```{r}
library(broom)
tidy(svm_matrix) %>%
  filter(term %in% c('accuracy','kappa','sensitivity','specificity')) %>%
  select(term, estimate, class) %>%
  pivot_wider(names_from = term, values_from = estimate) %>%
  #accuracy and kappa are only applicable to the whole model, but that comes out as NA in this tidy setup
  mutate(class = ifelse(is.na(class), "model summary", class))
```
This may be easier to view, or with a large number of categories, you could do something like graph sensitivity vs specificity.

## Tuning and Improving SVM
  
### We can try to improve results in several ways
 * hyperparameter tuning -- maybe a different cost value (C) would help by allowing softer or harder boundaries
 * changing the type of kernel -- for example, trying a gaussian (radial) kernel with the method "svmRadialCost"
 * collecting more data points (more posts = better ratio of observations to features)
 * adjusting the feature set -- fewer words, particular words, additional cleaning, finding relevant flags, or consolidating features even further with a method like PCA, or trying to find important features through a method like decision trees/random forest
 
# Other data types with SVM
Anything that can be expressed with numeric feature in a wide dataframe can be analyzed with SVM.  This method has been used successfully to identify handwritten letters or numbers, as long as those images are converted into a series of numeric features first. You could similarly analyze still images of athletic position, for example, as long as the information could be coded numerically (perhaps: angles, distances, heights, etc). 

