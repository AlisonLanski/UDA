---
title: "Topic Modeling"
author: "Unstructured Data Analytics"

output: 
  html_document:
    toc: true
    toc_float: true
    theme: spacelab
    highlight: tango
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

### 1. A Simple Topic Model
The `topicmodels` package provides us with a set of tools for topic modeling. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(stringr)
library(tidyr)
library(tidytext)
library(topicmodels)
```

Let's begin with a simple corpus of sentences (documents to us).

```{r}
corpus <-
  c(
    "Due to bad loans, the bank agreed to pay the fines.",
    "If you are late paying your bank loan, you will incur a fine.",
    "I heard that a new restaurant opened downtown.",
    "We had some good food at the new restaurant that just opened on Warwick street.",
    "How will you pay off the business loan for your restaurant?"
  )
```

The first thing we do is convert the corpus to a tibble.

```{r}
corpus <- tibble(document=1:5, text = corpus)
corpus
```

Then, we tokenize, remove stop words and convert it to a DTM (Document Term Matrix) using the `cast_dtm()` function from the `tidytext` package. 

```{r}
corpus_dtm <- corpus %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words, by = "word") %>%
  count(document, word) %>%
  ungroup() %>%
  cast_dtm(document, word, n)
```

Note that we converted our data to a DTM because that is a requirement of the `LDA()` function from the `topicmodels` package. Once our data has been converted to a DTM, we can no longer preview it by simply calling it. We need to pass it to the `as.matrix()` function to preview it.

```{r}
as.matrix(corpus_dtm)
```

**Question:** What vector space model representation is this - binary, frequency count or weighted vector?

Now that we have our data in the format that we need it, we can use the `LDA()` function for topic modeling. The `LDA()` function takes several arguments:

1. `k` - The number of topics we want. We set this to `2`.
2. `method` - The method to use when fitting the model. We set this to `"Gibbs"` for Gibbs Sampling. The other supported method is `"VEM"`.
3. `alpha` - The dirichlet prior for the probability that a document belongs to a topic. We set this to `0.1`. 
4. `delta` - The dirichlet prior for the probability distribution of words over topics. Set this to `0.1` as well.
5. `seed` - Random number generation seed. We set this to `1`.

```{r}
corpus_lda <- corpus_dtm %>%
  LDA(k = 2, method = "Gibbs", control = list(alpha = 1, delta = 0.1, seed = 1))

corpus_lda3 <- corpus_dtm %>%
  LDA(k = 3, method = "Gibbs", control = list(alpha = 1, delta = 0.1, seed = 1))

corpus_lda
```

With our model in place, let's do some exploration and interpretation. The `LDA()` function returns two matrices. The first is called `beta` and it contains the probability of words belonging to a topic. The second matrix is called `gamma` and it contains the prevalence of topics within a document. To work with these matrices directly, we need to tidy them.

Let's start with `beta`.

```{r}
corpus_topics <- corpus_lda %>%
  tidy(matrix = "beta")

corpus_topics %>% head()
```
```{r}
#EXTRA
#check that sum(beta) = 1

corpus_topics %>% group_by(term) %>% summarize(total = sum(beta))

corpus_topics %>% filter(term == "restaurant")

corpus_topics %>% arrange(topic, desc(beta)) %>% head(10) %>% select(beta) %>% colSums()

```

To see how good of a model we have, let's take a look at the top 5 words for each topic.

```{r}
corpus_topics %>%
  mutate(topic = str_c("topic", topic)) %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta) %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(mapping = aes(x = term, y = beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()
```

**Question:** What labels would you assign to each topic?

Next, we look at `gamma`.

```{r}
corpus_documents <- corpus_lda %>%
  tidy(matrix = "gamma")

corpus_documents

corpus_documents %>% filter(topic == 1) %>% select(topic, document, gamma)

corpus_documents %>% filter(document == 1  | document == 2 | document == 3) %>% select(document, topic, gamma) %>% 
  arrange(document)

tuning <- ldatuning::FindTopicsNumber(dtm = ap_dtm, 
                                      topics = 2:10,
                                      )
ldatuning::FindTopicsNumber_plot(tuning)

corpus_test <- tibble(document = 1:2,
                      text = c("I need a new loan to finance my house",
                               "Did you see there's a new place opening on Warwick?")) %>%
  unnest_tokens(output = word, input = text) %>%
  anti_join(stop_words, by = "word") %>%
  count(document, word) %>%
  ungroup() %>%
  cast_dtm(document = document, term = word, value = n)


perplexity(object = corpus_lda, newdata = corpus_test)
perplexity(object = corpus_lda3, newdata = corpus_test)



as.numeric(logLik(corpus_lda))
as.numeric(logLik(corpus_lda3))
```

Let's reformat this in a way that's easier to analyze.

```{r}
corpus_documents %>%
  mutate(topic = str_c("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = gamma) %>%
  mutate(document = as.integer(document)) %>%
  inner_join(corpus, by = "document") %>%
  arrange(document)
```

**Question:** How does this compare to what you expected?

### 2. Modeling the Topics from the Associated Press Dataset
To further illustrate topic modeling, let's work with a larger dataset (the Associated Press practice dataset from 1992). The dataset is provided by the `tm` package. It contains 10473 terms across 2246 documents.

```{r}
library(tm)
```

To import the data into our environment, we pass the name of the dataset ("AssociatedPress") to the `data()` function.

```{r}
data("AssociatedPress")
#i = document id
#j = term id
#v = count? not sure
```

Next, we remove the stop words and convert our dataset into a DTM.

```{r}
ap_dtm <- AssociatedPress %>%
  tidy() %>%
  anti_join(stop_words, by = c("term" = "word")) %>%
  cast_dtm(document = document, term = term, value = count)
```

Then we model the topics in the document.

```{r}
ap_lda <- ap_dtm %>%
  LDA(k = 4, method = "Gibbs", control = list(alpha = .1, delta = .1, seed = 1))

ap_ldad <- ap_dtm %>%
  LDA(k = 4, method = "Gibbs", control = list(alpha = .1, delta = 1, seed = 1))

ap_lda
```

#### Word-topic Probabilities
We can get the per-topic-per-word probabilities (`beta`)...

```{r}
ap_topics <- ap_lda %>%
  tidy(matrix = "beta")

ap_topics

ap_topics_d <- ap_ldad %>%
  tidy(matrix = "beta")

ap_topics_d
ap_topics %>% filter(topic == 1) %>% summarize(sum(beta, na.rm=T))

```

...and take a look at the terms that are most common within each topic.

```{r}
ap_topics %>%
  mutate(topic = str_c("topic", topic)) %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta) %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(mapping = aes(x = term, y = beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()
```

**Question:** What label would you assign to each topic?


#### Document-topic Probabilities
Now that we've seen the word probabilities by topic, let's get the per-document-per-topic probabilities (`gamma`).

```{r}
ap_documents <- ap_lda %>%
  tidy(matrix = "gamma")

ap_documents
```

First, we look at the documents with the most dominance by a single topic.

```{r}
ap_documents %>%
  mutate(topic = str_c("topic", topic)) %>%
  spread(topic, gamma) %>%
  arrange(desc(abs(topic1 - topic2)))
```

Look for a document that is almost entirely from a single topic (the gamma value is close to 1). Let's look at the most frequent words in that document.

```{r}
tidy(AssociatedPress) %>%
  filter(document ==2345) %>%
  arrange(desc(count))
```

Find another document that is mostly from a different topic. Let's take a look at it as well.

```{r}
tidy(AssociatedPress) %>%
  filter(document == 1234) %>%
  arrange(desc(count))
```

**Question:** Now that we see the words in some of these documents, do you still think our topic labels are good?

When we feel that we're done, we can return the consensus topic for each document, by taking the topic with the highest gamma value.

```{r}
ap_documents %>%
  group_by(document) %>%
  top_n(1, gamma) %>%
  ungroup() %>%
  arrange(as.integer(document))
```

The consensus topic can now serve as the label for each of the documents in our corpus. However, remember that topic modeling is soft clustering. A document can belong to more than one topic.


```{r}
#check K values

#read in the raw AP text, downloaded from this site: https://github.com/Blei-Lab/lda-c/blob/master/example/ap.tgz

dat <- read.delim("C:/Users/alanski/Downloads/ap.tgz", col.names = "ap")
dat2 <- dat  %>%
  mutate(docno = str_extract(ap, pattern = "(?<=<DOCNO> )AP.*(?= </DOCNO>)")) %>%
  mutate(docno = ifelse(is.na(docno) & str_detect(ap, pattern = "</DOC>"),
                        "doc_end", docno)) %>%
  tidyr::fill(docno, .direction = "down") %>%
  filter(!is.na(docno) & docno != "doc_end") %>%
  filter(!str_detect(ap, pattern = docno)) %>%
  filter(!str_detect(ap, pattern = ".{1,2}TEXT>$")) %>%
  #also, let's remove numbers 
  mutate(ap = str_remove_all(ap, pattern = "[:digit:]+"))


possible_k <- c(2:20)
set.seed(21)
k_train_index <- sample(1:nrow(dat2), size = 1000, replace = F)
k_train <- dat2[k_train_index, ] %>%
  unnest_tokens(word, ap) %>%
  anti_join(stop_words, by = "word") %>%
  count(docno, word) %>%
  ungroup() %>%
  cast_dtm(document = docno, term = word, value = n)
k_test <- dat2[-k_train_index, ][1:200,] %>%
    unnest_tokens(word, ap) %>%
  anti_join(stop_words, by = "word") %>%
  count(docno, word) %>%
  ungroup() %>%
  cast_dtm(document = docno, term = word, value = n)

perplexity_df <- data.frame(train = numeric(),
                            test = numeric())
for(k in possible_k){
  fitted <- LDA(k_train, k = k, method = "Gibbs", 
                control=list(alpha = 0.1, delta = 0.1, seed = 1))
  perplexity_df[k,1] <- perplexity(fitted, newdata = k_train)
  perplexity_df[k,2]  <- perplexity(fitted, newdata = k_test) 
}

perplexity_df %>%
  filter(!is.na(train)) %>%
  mutate(topics = row_number()+1) %>%
  pivot_longer(-topics, names_to = 'source', values_to = 'perplexity') %>%
  #mutate(perplexity = test-train) %>%
  ggplot(
    aes(x = topics, y = perplexity, color = source)
  ) +
  geom_line(size = 2)


perplexity(LDA(k_train, k = 10, method = "Gibbs", 
                control=list(alpha = 0.1, delta = 0.1, seed = 1)), 
           newdata = k_test)

ap_lda <- ap_dtm %>%
  LDA(k = 4, method = "Gibbs", control = list(alpha = .1, delta = .1, seed = 1))

ap_ldad <- ap_dtm %>%
  LDA(k = 4, method = "Gibbs", control = list(alpha = .1, delta = 1, seed = 1))


library(topicdoc)

topicdoc::topic_coherence(topic_model = corpus_lda, dtm_data = corpus_dtm)
topicdoc::topic_coherence(topic_model = ap_lda, dtm_data = ap_dtm)
topicdoc::topic_exclusivity(corpus_lda3)

topicdoc::topic_diagnostics(topic_model = corpus_lda, dtm_data = corpus_dtm)
ap_diag <- topicdoc::topic_diagnostics(topic_model = ap_lda, dtm_data = ap_dtm)

topic_words <- data.frame(terms(ap_lda, 5)) %>%
  pivot_longer(cols = 1:4, names_to = "topic", values_to = "word") %>%
  mutate(topic = as.integer(str_extract(topic, "[0-9]+"))) %>%
  group_by(topic) %>%
  mutate(words = paste(word, collapse = ', ')) %>%
  ungroup() %>%
  mutate(words = paste('Topic', topic, '-', words)) %>%
  select(topic, words) %>%
  distinct()


ap_diag %>%
  pivot_longer(-topic_num, names_to = "diagnostics", values_to = "value") %>%
  left_join(topic_words, by = c("topic_num" = "topic"))  %>%
  ggplot(
    aes(x = topic_num, y = value, fill = words)
  ) +
  geom_bar(stat = "identity") +
  facet_wrap(~diagnostics, scales = "free")

  

```
```{r}
top_terms <- terms(ap_lda, 10)
top_terms_vec <- unique(as.character(top_terms))


  # Coerce document-term matrix to simple triplet matrix
  dtm_data <- slam::as.simple_triplet_matrix(ap_dtm)

coherence <- function(dtm_data, top_terms, smoothing_beta){
  # Get the relevant entries of the document-term matrix
  rel_dtm <- dtm_data[,top_terms]

  # Turn it into a logical representing co-occurences
  df_dtm <- rel_dtm > 0

  # Calculate document frequencies for each term and all of its co-occurences
  cooc_mat <- tcrossprod_simple_triplet_matrix(t(df_dtm))

  # Quickly get the number of top terms for the for-loop below
  top_n_tokens <- length(top_terms)

  # Using the syntax from the paper, calculate coherence
  c_l <- 0
  for (m in 2:top_n_tokens) {
    for (l in 1:(top_n_tokens - 1)) {
      df_ml <- cooc_mat[m,l]
      df_l <- cooc_mat[l,l]
      c_l <- c_l + log((df_ml + smoothing_beta) / df_l)
    }
  }

  c_l
}
  
  # Apply coherence calculation to all topics' top terms
  unname(apply(top_terms, 2, coherence, dtm_data = dtm_data, smoothing_beta = 1))
```

