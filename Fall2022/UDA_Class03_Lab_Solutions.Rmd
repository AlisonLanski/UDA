---
title: "Analyzing Text Preliminaries: Lab"
author: "Unstructured Data Analytics"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: spacelab
    highlight: tango
    df_print: paged
  always_allow_html: true
  pdf_document:
    toc: yes
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

Set up packages
```{r echo=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(gutenbergr)
library(tidytext)
```

Download "War of the Worlds"
```{r ww_download}
war_worlds <- gutenberg_download(36, mirror = 'https://gutenberg.pglaf.org/')
```

## Using concepts and skills from the previous class, try to answer the following questions

### 1. What are the 10 most common words in this book overall?
How do we solve this?
> Break the book into words
> Count the words
> Find the top 10

```{r full_count}
#one way to do it
war_worlds %>% 
  unnest_tokens(output = word, input = text, token = "words") %>%
  count(word) %>%
  arrange(desc(n)) %>%
  head(10)  

#other functions you might use include
#slice, slice_max, top_n, group_by, summarize() using n() to get rowcounts
  
```

### 2. What are the 10 most common words in this book after removal of stopwords?
Same process as above, but we have to remove stopwords after we tokenize
```{r no_stopwords_count}
#one way to do it
war_worlds %>% 
  unnest_tokens(output = word, input = text, token = "words") %>%
  anti_join(stop_words, by = "word") %>%   
  group_by(word) %>%
  summarize(wordcount = n()) %>%
  arrange(desc(wordcount)) %>%
  head(10)  

#removing stopwords can happen before or after you count but is more efficient to do early

```

Download more HG Wells books
```{r Wells_download}
#with multiple books, it's nice to also pull the title
hgwells <- gutenberg_download(c(35, 36, 159, 5230), 
                              meta_fields = "title", 
                              mirror = 'https://gutenberg.pglaf.org/')
```

### 3. Which books we did get?
```{r download_titles}
hgwells %>% 
  distinct(title)
```


### 4. What are the most frequent words in each title, after removing stopwords?  Do they seem related at all to subject matter suggested by the title? What do you think the books might be about, based on these words?
How do we break this down?   
We still have to tokenize into words  
We want to count the words within each book (12 "fun" in book 1, 23 "fun" in book 2)  
So we have to group by book and by word, instead of only grouping by word  
```{r count_by_title}
#one way to do it
hgwells %>%
  unnest_tokens(word, text, "words") %>%
  anti_join(stop_words, by = "word") %>%
  group_by(title, word) %>%
  summarize(wordcount = n()) %>%
  arrange(desc(wordcount)) %>%
  #use a dplyr method that will operate once per group, like
  slice(1:10)
```

### 5. What are the most frequent bigrams in each title, including stopwords? Does this seem "better" to you than your results of single-words?
This is the same general process, but we have to ask for bigram tokens insteaad of single word tokens.  And for clarity, it's nice to update the column names for the output and the count.
```{r bigram_by_title}
hgwells %>%
  unnest_tokens(output = bigram, input = text, token= "ngrams", n = 2) %>%
  group_by(title, bigram) %>%
  summarize(bigram_count = n()) %>%
  arrange(desc(bigram_count)) %>%
  #use a dplyr method that will operate once per group, like
  slice(1:10)
```

### 6. Create a reduced dataframe that only has words occurring more than 25 times in War of the Worlds
And... for a good result, it's nice to keep out the stopwords
```{r smaller_data}
war_26 <- war_worlds %>% 
  unnest_tokens(output = word, input = text, token = "words") %>%
  anti_join(stop_words, by = "word") %>%   
  group_by(word) %>%
  summarize(wordcount = n()) %>%
  arrange(desc(wordcount)) %>%
  filter(wordcount > 25)  

#check it
war_26 %>% tail()
```


### 7. What are some types of visualizations that might help represent the information in war_26?

 -Word clouds  
 -Bar charts  
 -Cumulative Distribution charts  

## Visualizing Text Counts
One of the most common methods is a wordcloud, which shows frequency through size.
There is a nice package for this which produced interactive html images (if you mouse over a word, you get the count).
You will probably have to install this package `wordcloud2`  

### Making a wordcloud
```{r wordcloud}
# load the package
library(wordcloud2)

# clean up the data so it's ready for the wordcloud2 package
# we have to have  a dataframe with only 2 columns (word and count)
# we have to remove any internal grouping structure

war_26 %>%
  # use select as needed to keep only the columns we want
  select(word, wordcount) %>%
  # remove the grouping structure from the object
  ungroup() %>%
  # feed that into the wordcloud
  # you can mess around with the argument values to see what happens
  wordcloud2(size = .5, shape = "circle")
  
  
```
  
### Making a bar graph (works better with fewer words)
We have to count the words first if we want to limit to the top words  
We can then graph whatever set of words we want  
This graph could be prettier or more useful, but it works  
There are other ways to set up and prep a bar graph also  
  
```{r barchart}
war_26 %>% 
  top_n(12) %>%
  ggplot(aes(x = word, y = wordcount)) +
  geom_bar(stat="identity")
```
  
### Making a cumulative distribution graph (easier to see with fewer words)
This graph-type is one way to look at a breakout of the total wordcount by individual words  
We have to count the words first if we want to limit to the top words  
We can then graph whatever set of words we want  
This graph could be prettier, but it works  
```{r cumulative}
#This graph also could use some cleanup

#This style of plot also needs a little more prep work, like...
# establishing counts
# setting up a cumulative count (in order) for the y-values
# setting up a grouping column for the x-values 
#   (The line won't be drawn by default since x is categories (not numbers).
#     To get the line to appear, we have to put the categories together in a group)

war_26 %>% #remember this data is already sorted and limited
  #we could limit it more if we want to be able to read the words

  mutate(cumulative = cumsum(wordcount), #this sets up a cumulative total
         title = "book") %>% #this is my grouping variable
  
  ggplot(aes(x = reorder(word, desc(wordcount)), #make sure that x is in order
             y = cumulative, 
             group = title)) +
  geom_point() + #add points and lines
  geom_line()
```

